import json
import os
import sys
import gzip
import concurrent.futures as cfuts

from tqdm import tqdm
import requests
import time
from typing import Dict, Any
import re

def talk_to_GPT(prompt: str, model: str = "gpt-4o-2024-08-06") -> Dict[str, Any]:
    """
    Interact with GPT-4, sending the user's prompt and returning the generated content along with the token usage.

    :param prompt: The user's prompt text
    :param model: The name of the model to be used, default is "gpt-4o-2024-08-06" or "gpt-4o-mini"
    :return: A dictionary containing the message generated by GPT-4 and the tokens consumed
    """

    assert model in [
        "gpt-3.5-turbo",
        "gpt-3.5-turbo-0125",
        "gpt-4-1106-preview",
        "gpt-4-0125-preview",
        "gpt-4-turbo-2024-04-09",
        "gpt-4-vision-preview",
        "gpt-4o",
        "gpt-4o-2024-05-13",
        "gpt-4o-2024-08-06",
        "gpt-4o-mini",
        "text-embedding-3-large",
        "text-embedding-3-small"
    ]
    data = {
        "model": model, 
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "stream": False
    }
    
    headers = {
        'Authorization': 'Bearer sk-E7gOgfTjf0tREnYXEa1767178b7f43499eBdA49389CdD905',  # Get API key from environment variable
        'Content-Type': 'application/json'
    }
    
    url = 'https://api2.road2all.com/v1/chat/completions'
    
    max_retries = 3
    timeout = 30  # seconds

    for attempt in range(max_retries):
        try:
            response = requests.post(url, json=data, headers=headers, timeout=timeout)
            response.raise_for_status()  # Check if the request was successful
            
            # Parse the response content
            response_data = response.json()
            message = response_data.get('choices')[0].get('message', {}).get('content', '')
            usage = response_data.get('usage', {})
            prompt_tokens = usage.get('prompt_tokens', 0)
            completion_tokens = usage.get('completion_tokens', 0)
            
            return {
                "message": message,
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "model": model
            }
        
        except requests.exceptions.RequestException as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            if attempt < max_retries - 1:
                print("Retrying...")
                time.sleep(2)  # Adding a small delay before retrying
            else:
                return {"error": str(e)}

    return {"error": "Request failed after multiple attempts"}

def save_json(data, file_path):
    """Save data to a JSON file."""
    with open(file_path, 'w') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def run_answers(datas, model):

    def parse_response(response):
        """
        Parse the response to extract JSON and Python code.
        """
        pattern = r"```(python)(.*?)```"
        matches = re.findall(pattern, response, re.DOTALL)
        for data_type, data_content in matches:
            data_content = data_content.strip()
        return data_content

    def get_response(data):
        default_prompt = """
        You are a Python engineer with a focus in astronomy, tasked with generating Python code in response to user instructions provided within the "instruction" field. Users might request tasks related to astronomy, typically requiring the utilization of astropy and astroquery, or general Python code generation tasks. This Python code will be directly executed, and we will provide execution results back to the user. Hence, you are expected to produce complete and executable Python code, entering it within the generate_function() function and using return for the outcomes. Consult the given example for guidance.

        **Output format**
        instruction:
        python_code：
        ```python
        ```

        **Example**
        instruction: Query the GLIMPSE infrared survey catalog for all infrared celestial bodies within a 2 arcminute square area centered on coordinates 13:16:43.64 -62:58:31.39.
        python_code：
        ```python
        def generate_function():
        from astroquery.ipac.irsa import Irsa
        import astropy.units as u
        Irsa.ROW_LIMIT = 10000
        table = Irsa.query_region("13:16:43.64 -62:58:31.39", catalog="glimpse_s07", spatial="Box", width=2*u.arcmin).to_pandas()
        return table
        ```

        **Task**
        instruction：>>> INSTRUCTION <<<
        python_code：
        """
        default_prompt = default_prompt.replace('>>> INSTRUCTION <<<', data['prompt_en'])
        response = talk_to_GPT(default_prompt)['message']
        code = parse_response(response)
        data['generate_code'] = code
        return data

    responses = []
    with cfuts.ThreadPoolExecutor(
        max_workers=32
    ) as executor:
        futs = []
        for data in datas:
            futs.append(executor.submit(get_response, data))

        for f in tqdm(cfuts.as_completed(futs), total=len(futs)):
            result = f.result()
            responses.append(result)
    # responses.sort(key=lambda x: int(x['id']))
    save_json(responses,f'data/{model}-answers.jsonl')

if __name__=='__main__':
    datas = json.load(open("data/dataset.json"))
    datas = datas[:10]
    run_answers(datas, 'gpt-4o-2024-08-06')